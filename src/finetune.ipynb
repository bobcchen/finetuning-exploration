{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22932b1a-22b7-4dfd-b4ef-d1722a18b90f",
   "metadata": {},
   "source": [
    "From https://github.com/facebookresearch/llama-recipes/\n",
    "\n",
    "specifically from commit 43771602c9d7808c888eb5995ccce4bc8beafb1f as of 1/12/2023\n",
    "\n",
    "# Model\n",
    "\n",
    "Download official Llama2 from Meta, specifically the 7b and 13b variants.\n",
    "\n",
    "Use https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py to convert to hf format\n",
    "\n",
    "Output folder at and `../model-7b` and `../model-13b` wrt this notebook\n",
    "\n",
    "# Dataset\n",
    "\n",
    "Download `samsun` dataset to `../dataset` wrt to this notebook\n",
    "\n",
    "```\n",
    "from datasets import load_dataset\n",
    "\n",
    "for split in ['train', 'validation']:\n",
    "    load_dataset(\"samsum\", split=split).to_json(f'../dataset/samsun_{split}.jsonl')\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff13700-1358-435b-9199-3bba34547df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetuning import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811dec98-391a-451a-8ab3-012561010b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19341fbf8e5452eb392d8eb5ce3a6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model ../model-7b\n",
      "\n",
      "--> ../model-7b has 262.41024 Million params\n",
      "\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7242dfbee5294a93a9a9b5ee340170e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ced83e7e07427aaade3c7e6f4ace1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training Set Length = 147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a9cb64d2a2459c886c5b31d9c4b23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2e2e73dee84ca9869488d9ac185061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Validation Set Length = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1:   0%|\u001b[34m                                                                         \u001b[0m| 0/36 [00:00<?, ?it/s]\u001b[0m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1/3, step 146/147 completed (loss: 0.14629891514778137): : 37it [04:04,  6.62s/it]                      6.64s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 5 GB\n",
      "Max CUDA memory reserved was 5 GB\n",
      "Peak active CUDA memory was 5 GB\n",
      "Cuda Malloc retires : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 5 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m███████████████████████████████████████████████████████████████████\u001b[0m| 8/8 [00:06<00:00,  1.26it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(3.2692, device='cuda:0') eval_epoch_loss=tensor(1.1845, device='cuda:0')\n",
      "best eval loss on epoch 1 is 1.1845430135726929\n",
      "Epoch 1: train_perplexity=1.4029, train_epoch_loss=0.3385, epoch time 245.15665548799916s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/3, step 146/147 completed (loss: 0.2730576992034912): : 37it [04:19,  7.01s/it]                       7.41s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 5 GB\n",
      "Max CUDA memory reserved was 5 GB\n",
      "Peak active CUDA memory was 5 GB\n",
      "Cuda Malloc retires : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 6 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m███████████████████████████████████████████████████████████████████\u001b[0m| 8/8 [00:06<00:00,  1.20it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(3.2262, device='cuda:0') eval_epoch_loss=tensor(1.1713, device='cuda:0')\n",
      "best eval loss on epoch 2 is 1.1713013648986816\n",
      "Epoch 2: train_perplexity=1.3081, train_epoch_loss=0.2686, epoch time 259.4338902799991s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/3, step 146/147 completed (loss: 0.33959704637527466): : 37it [04:24,  7.16s/it]                      7.37s/it]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max CUDA memory allocated was 5 GB\n",
      "Max CUDA memory reserved was 6 GB\n",
      "Peak active CUDA memory was 5 GB\n",
      "Cuda Malloc retires : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 6 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating Epoch: 100%|\u001b[32m███████████████████████████████████████████████████████████████████\u001b[0m| 8/8 [00:06<00:00,  1.20it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eval_ppl=tensor(3.4685, device='cuda:0') eval_epoch_loss=tensor(1.2437, device='cuda:0')\n",
      "Epoch 3: train_perplexity=1.2570, train_epoch_loss=0.2287, epoch time 265.19161016s\n",
      "Key: avg_train_prep, Value: 1.3226686716079712\n",
      "Key: avg_train_loss, Value: 0.2786160707473755\n",
      "Key: avg_eval_prep, Value: 3.321303129196167\n",
      "Key: avg_eval_loss, Value: 1.1757153272628784\n",
      "Key: avg_epoch_time, Value: 256.59405197599943\n",
      "Key: avg_checkpoint_time, Value: 2.54233297406851e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'model_name': '../model-7b',\n",
    "    'dataset_cache_dir': '../dataset',\n",
    "    'split_slice': '1%', # use only split_slice of each split, to prevent oom on local\n",
    "    'use_peft': True,\n",
    "}\n",
    "\n",
    "main(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c003f47-9d0e-43d4-9d67-3b296b7e1e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:29<00:00,  9.99s/it]\n",
      "--> Model ../model-7b\n",
      "\n",
      "--> ../model-7b has 262.41024 Million params\n",
      "\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "--> Training Set Length = 147\n",
      "--> Validation Set Length = 8\n",
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1:   0%|\u001b[34m                                 \u001b[0m| 0/36 [00:00<?, ?it/s]\u001b[0m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Training Epoch: 1/3, step 146/147 completed (loss: 0.14629891514778137): : 37it [03:49,  6.20s/it]\n",
      "Max CUDA memory allocated was 5 GB\n",
      "Max CUDA memory reserved was 5 GB\n",
      "Peak active CUDA memory was 5 GB\n",
      "Cuda Malloc retires : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n",
      "evaluating Epoch: 100%|\u001b[32m███████████████████████████\u001b[0m| 8/8 [00:06<00:00,  1.28it/s]\u001b[0m\n",
      " eval_ppl=tensor(3.2692, device='cuda:0') eval_epoch_loss=tensor(1.1845, device='cuda:0')\n",
      "best eval loss on epoch 1 is 1.1845430135726929\n",
      "Epoch 1: train_perplexity=1.4029, train_epoch_loss=0.3385, epoch time 229.48925341500035s\n",
      "Training Epoch: 2/3, step 146/147 completed (loss: 0.2730576992034912): : 37it [04:12,  6.83s/it] \n",
      "Max CUDA memory allocated was 5 GB\n",
      "Max CUDA memory reserved was 5 GB\n",
      "Peak active CUDA memory was 5 GB\n",
      "Cuda Malloc retires : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n",
      "evaluating Epoch: 100%|\u001b[32m███████████████████████████\u001b[0m| 8/8 [00:06<00:00,  1.22it/s]\u001b[0m\n",
      " eval_ppl=tensor(3.2262, device='cuda:0') eval_epoch_loss=tensor(1.1713, device='cuda:0')\n",
      "best eval loss on epoch 2 is 1.1713013648986816\n",
      "Epoch 2: train_perplexity=1.3081, train_epoch_loss=0.2686, epoch time 252.7524484430014s\n",
      "Training Epoch: 3/3, step 146/147 completed (loss: 0.33959704637527466): : 37it [04:12,  6.81s/it]\n",
      "Max CUDA memory allocated was 5 GB\n",
      "Max CUDA memory reserved was 6 GB\n",
      "Peak active CUDA memory was 5 GB\n",
      "Cuda Malloc retires : 0\n",
      "CPU Total Peak Memory consumed during the train (max): 2 GB\n",
      "evaluating Epoch: 100%|\u001b[32m███████████████████████████\u001b[0m| 8/8 [00:06<00:00,  1.22it/s]\u001b[0m\n",
      " eval_ppl=tensor(3.4685, device='cuda:0') eval_epoch_loss=tensor(1.2437, device='cuda:0')\n",
      "Epoch 3: train_perplexity=1.2570, train_epoch_loss=0.2287, epoch time 252.3056405970001s\n",
      "Key: avg_train_prep, Value: 1.3226686716079712\n",
      "Key: avg_train_loss, Value: 0.2786160707473755\n",
      "Key: avg_eval_prep, Value: 3.321303129196167\n",
      "Key: avg_eval_loss, Value: 1.1757153272628784\n",
      "Key: avg_epoch_time, Value: 244.8491141516673\n",
      "Key: avg_checkpoint_time, Value: 1.5420003668017064e-06\n"
     ]
    }
   ],
   "source": [
    "# alternatively use cli\n",
    "!python3 finetuning.py \\\n",
    "    --model_name ../model-7b \\\n",
    "    --dataset_cache_dir ../dataset \\\n",
    "    --split_slice 1% \\\n",
    "    --use_peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d680e-5dfa-46c0-9ff2-ff45e4fbc034",
   "metadata": {},
   "source": [
    "# Alternative runs\n",
    "\n",
    "## Single V100 (32GB) GPU\n",
    "\n",
    "LoRA PEFT of quantized 7b (`load_in_4bit=True`) in half precision works as per local\n",
    "\n",
    "```\n",
    "python3 finetuning.py \\\n",
    "    --split_slice 1% \\\n",
    "    --use_peft \\\n",
    "    --quantization True \\\n",
    "    --use_fp16 True\n",
    "```\n",
    "\n",
    "Full finetuning of 7b in full/half precision results in CUDA OOM\n",
    "\n",
    "```\n",
    "python3 finetuning.py \\\n",
    "    --split_slice 1% \\\n",
    "    --quantization False \\\n",
    "    --use_fp16 False\n",
    "```\n",
    "```\n",
    "python3 finetuning.py \\\n",
    "    --split_slice 1% \\\n",
    "    --quantization False \\\n",
    "    --use_fp16 True\n",
    "```\n",
    "\n",
    "Full finetuning of quantized 7b (`load_in_4bit=True`) in full precision works, but training loss does not decrease\n",
    "\n",
    "```\n",
    "python3 finetuning.py \\\n",
    "    --split_slice 1% \\\n",
    "    --quantization True \\\n",
    "    --use_fp16 False \n",
    "```\n",
    "\n",
    "Full finetuning of quantized 7b (`load_in_4bit=True`) in half precision results in\n",
    "\n",
    "`AssertionError: No inf checks were recorded for this optimizer.`\n",
    "\n",
    "https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-accumulation\n",
    "\n",
    "\n",
    "```\n",
    "python3 finetuning.py \\\n",
    "    --split_slice 1% \\\n",
    "    --quantization True \\\n",
    "    --use_fp16 True \n",
    "```\n",
    "\n",
    "## Single node multi V100 (32GB) GPU\n",
    "\n",
    "FSDP is used to do sharding of model (and data) across gpus.\n",
    "\n",
    "~~There was a weird issue of the device not setting properly for FSDP, resulting in `Inconsistent compute_device and device_id`, possibly fix it with `export CUDA_VISIBLE_DEVICES=0,1,2,3`.~~ It is due to `quantization` set to `True`, resulting in `device_map` to be `None`, which is possibly a bug in the code. Do not use quantization when using FSDP, it is not supported anyway.\n",
    "\n",
    "Full finetuning of 7b in half precision results in CUDA OOM\n",
    "\n",
    "```\n",
    "torchrun \\\n",
    "    --nnodes 1 \\\n",
    "    --nproc_per_node 4 \\\n",
    "    finetuning.py \\\n",
    "    --split_size 1% \\\n",
    "    --enable_fsdp \\\n",
    "    --quantization False \\\n",
    "    --use_fp16 True \\\n",
    "```\n",
    "\n",
    "LoRA PEFT of 7b in half precision works! Note that quantization is not supported for FSDP.\n",
    "\n",
    "eval_ppl and eval_loss decreases similarly to the single gpu case as well.\n",
    "\n",
    "```\n",
    "torchrun \\\n",
    "    --nnodes 1 \\\n",
    "    --nproc_per_node 4 \\\n",
    "    finetuning.py \\\n",
    "    --split_size 1% \\\n",
    "    --enable_fsdp \\\n",
    "    --use_peft \\\n",
    "    --quantization False \\\n",
    "    --use_fp16 True \\\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61b279-294f-4e4e-b6c8-9730a7df66ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
